{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmg1xq2JnMtl"
      },
      "source": [
        "## Intro\n",
        "As a newly hired machine learning (ML) engineer, you have been tasked with improving 'Cat-titude Enhancer 2000,' a product that utilizes technology to monitor and enhance the well-being of cats through a play gym. The system operates by continuously streaming video footage from a camera installed in the gym, which provides input to your ML model. Based on this data, your system generates an overall recommended recipe with specific calorie counts for the cat to promote happiness, health, and strength.\n",
        "\n",
        "## Objectives\n",
        "You only deal with certain specifc backend tasks. These are your high level objectives.\n",
        "1. Improve cat image classification system. The initial cat classification model was not trained properly and this lead to some complaints from your senior ML engineer.\n",
        "2. Find a good threshold for cat image classification system. Earlier version of the model detected cat when there were none, the system ended up recommending that the cat be fed more food. Our product is being accused of producing fat cats. We must do something about it.\n",
        "3. Develop the prototype to take a 'glamour shot' of the cat with a still frame with blurred background. We get internet points if we take an amazing shot of the cat from the video and send it to owner.\n",
        "\n",
        "\n",
        "We will be using real world data, so your solutions will not be perfect. You will get full mark as long as you prove that you are capable of implementing solutions that uses the computer vision techniques and justify your answers in the context of what is asked.\n",
        "\n",
        "## Questions\n",
        "On to assignment proper. Your tasks are:\n",
        "1. Your predecessor build a training algorithm that just trained for a random number of epochs and assumed that it was the best model. Build an early stopping system for this training, ie, find the number of epochs to train, when the value of a selected quality metric is maximized. Choose an appropriate metric and justify your explanation. [10 marks]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HHtTOEMoLuG"
      },
      "source": [
        "This is the code written by your predecessor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: torch in /home/cifedior/.local/lib/python3.10/site-packages (2.2.1)\n",
            "Requirement already satisfied: torchvision in /home/cifedior/.local/lib/python3.10/site-packages (0.17.1)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/cifedior/.local/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/cifedior/.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: sympy in /usr/lib/python3/dist-packages (from torch) (1.9)\n",
            "Requirement already satisfied: networkx in /home/cifedior/.local/lib/python3.10/site-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/cifedior/.local/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/cifedior/.local/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/cifedior/.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/cifedior/.local/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/cifedior/.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/cifedior/.local/lib/python3.10/site-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/cifedior/.local/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch) (3.0.3)\n",
            "Requirement already satisfied: triton==2.2.0 in /home/cifedior/.local/lib/python3.10/site-packages (from torch) (2.2.0)\n",
            "Requirement already satisfied: fsspec in /home/cifedior/.local/lib/python3.10/site-packages (from torch) (2024.2.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/cifedior/.local/lib/python3.10/site-packages (from torch) (2.19.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /home/cifedior/.local/lib/python3.10/site-packages (from torch) (4.10.0)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/cifedior/.local/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: filelock in /home/cifedior/.local/lib/python3.10/site-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/cifedior/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.99)\n",
            "Requirement already satisfied: numpy in /home/cifedior/.local/lib/python3.10/site-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/lib/python3/dist-packages (from torchvision) (9.0.1)\n"
          ]
        }
      ],
      "source": [
        "# Install touch and touch vision\n",
        "\n",
        "!pip3 install torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "G9c4ErXSqJ2t"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import mobilenet_v2\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "suKK_wP2SeX-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/cifedior/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/home/cifedior/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1,   156] loss: 0.233\n",
            "[2,   156] loss: 0.111\n"
          ]
        }
      ],
      "source": [
        "# Download and load the dataset\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "# Filter the dataset to only include images of cats (class 3) and another class (e.g., class 0)\n",
        "dataset = [(img, label) for img, label in dataset if label in [0, 3]]\n",
        "trainset = dataset\n",
        "# Make dataloader\n",
        "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Load the pre-trained MobileNetV2 model and modify last layer\n",
        "model = mobilenet_v2(pretrained=True)\n",
        "model.classifier[1] = nn.Linear(model.last_channel, 2)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Train the model and track metrics with MLflow\n",
        "softmax = nn.Softmax(dim=1)\n",
        "for epoch in range(2):  # loop over the dataset multiple times\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        labels = torch.where(labels==3, torch.tensor([1]), torch.tensor([0]))  # convert labels to 0/1\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # print(\"outputs.shape >> \", outputs.shape)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 156 == 155:    # print every 2000 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 156))\n",
        "            running_loss = 0.0\n",
        "best_model = model\n",
        "THRESHOLD = 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The metric to be utilized for early stopping is the validation loss. As the validation loss begins to deteriorate, the training process is halted after reaching a certain level of patience."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "[1,   125] loss: 32.861\n",
            "[Epoch 1] Validation Loss: 5.843\n",
            "counter >>  0\n",
            "[2,   125] loss: 15.565\n",
            "[Epoch 2] Validation Loss: 5.080\n",
            "counter >>  0\n",
            "[3,   125] loss: 9.955\n",
            "[Epoch 3] Validation Loss: 5.261\n",
            "counter >>  1\n",
            "[4,   125] loss: 7.310\n",
            "[Epoch 4] Validation Loss: 5.365\n",
            "counter >>  2\n",
            "[5,   125] loss: 5.601\n",
            "[Epoch 5] Validation Loss: 5.679\n",
            "counter >>  3\n",
            "[6,   125] loss: 5.702\n",
            "[Epoch 6] Validation Loss: 5.637\n",
            "counter >>  4\n",
            "[7,   125] loss: 4.845\n",
            "[Epoch 7] Validation Loss: 5.085\n",
            "counter >>  5\n",
            "Early stopping triggered. Stopping training.\n"
          ]
        }
      ],
      "source": [
        "## Your code here\n",
        "\n",
        "# Download and load the dataset\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "# Filter the dataset to only include images of cats (class 3) and another class (e.g., class 0)\n",
        "dataset = [(img, label) for img, label in dataset if label in [0, 3]]\n",
        "trainset = dataset\n",
        "\n",
        "# Split dataset into training and validation sets\n",
        "#Validation set is used to calculate early stoppage\n",
        "train_size = int(0.8 * len(trainset))\n",
        "val_size = len(trainset) - train_size\n",
        "train_dataset, val_dataset = random_split(trainset, [train_size, val_size])\n",
        "\n",
        "# Create DataLoader for training and validation sets\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "\n",
        "# Initialize variables for early stopping\n",
        "#starts validation loss at infinity\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "# Patience represents the number of epochs to wait \n",
        "# before stopping training when the validation loss does not improve\n",
        "patience = 5\n",
        "counter = 0\n",
        "\n",
        "# Load the pre-trained MobileNetV2 model and modify last layer\n",
        "model = mobilenet_v2(pretrained=True)\n",
        "model.classifier[1] = nn.Linear(model.last_channel, 2)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Save best model\n",
        "best_model = model\n",
        "\n",
        "# Train the model and track metrics with early stopping\n",
        "# Maximum number of epoch is 100\n",
        "for epoch in range(100):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "\n",
        "        # convert labels to 0/1\n",
        "        labels = torch.where(labels==3, torch.tensor([1]), torch.tensor([0]))\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # print every 125 mini-batches (basically print once every epoch)\n",
        "        if i % 125 == 124:\n",
        "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss))\n",
        "    \n",
        "    # Validate the model after each epoch\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad(): # Don't compute gradient for this operation\n",
        "        for val_data in val_loader:\n",
        "            val_inputs, val_labels = val_data\n",
        "            val_labels = torch.where(val_labels==3, torch.tensor([1]), torch.tensor([0]))\n",
        "            \n",
        "            val_outputs = model(val_inputs)\n",
        "            val_loss += criterion(val_outputs, val_labels).item()\n",
        "    \n",
        "            \n",
        "    print('[Epoch %d] Validation Loss: %.3f' % (epoch + 1, val_loss))\n",
        "    \n",
        "    # Check for early stopping\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        counter = 0  # Reset counter if validation loss improves\n",
        "        best_model = model  # Update the best model\n",
        "    else:\n",
        "        counter += 1  # Increment counter if validation loss does not improve\n",
        "    \n",
        "    print('counter >> ', counter)\n",
        "    # Check if early stopping criteria met\n",
        "    if counter >= patience:\n",
        "        print(\"Early stopping triggered. Stopping training.\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "only integer tensors of a single element can be converted to an index",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[11], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m predicted_labels \u001b[38;5;241m=\u001b[39m (probabilities[:, \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m THRESHOLD)\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Convert training labels and predicted labels to NumPy arrays\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m training_labels_np \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mflatten()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     31\u001b[0m predicted_labels_np \u001b[38;5;241m=\u001b[39m predicted_labels\u001b[38;5;241m.\u001b[39mflatten()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Compute confusion matrix\u001b[39;00m\n",
            "\u001b[0;31mTypeError\u001b[0m: only integer tensors of a single element can be converted to an index"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "softmax = nn.Softmax(dim=1)\n",
        "\n",
        "# Compute training outputs and labels using the best model\n",
        "best_training_outputs = []\n",
        "training_labels = []\n",
        "for inputs, labels in train_loader:\n",
        "    outputs = best_model(inputs)\n",
        "    best_training_outputs.append(outputs)\n",
        "    training_labels.append(labels)\n",
        "\n",
        "# Compute validation outputs and labels using the best model\n",
        "best_validation_outputs = []\n",
        "validation_labels = []\n",
        "for inputs, labels in val_loader:\n",
        "    outputs = best_model(inputs)\n",
        "    best_validation_outputs.append(outputs)\n",
        "    validation_labels.append(labels)\n",
        "\n",
        "best_training_outputs_tensor = torch.cat(best_training_outputs, dim=0)\n",
        "\n",
        "probabilities = softmax(best_training_outputs_tensor)\n",
        "\n",
        "# Convert probabilities to predicted labels using threshold\n",
        "predicted_labels = (probabilities[:, 1] > THRESHOLD).long()\n",
        "\n",
        "# Convert training labels and predicted labels to NumPy arrays\n",
        "training_labels_np = torch.tensor(training_labels).flatten().numpy()\n",
        "predicted_labels_np = predicted_labels.flatten().numpy()\n",
        "\n",
        "# Compute confusion matrix\n",
        "conf_matrix = confusion_matrix(training_labels_np, predicted_labels_np)\n",
        "\n",
        "# Plot confusion matrix using seaborn heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title(f'Confusion Matrix - Training Set (Threshold={THRESHOLD})')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "evxz0bLaKWOH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.04774435609579086"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Your code here\n",
        "for i, data in enumerate(trainloader, 0):\n",
        "    inputs, labels = data\n",
        "\n",
        "    if i == 0:\n",
        "        print(len(inputs[0]))\n",
        "\n",
        "    labels = torch.where(labels==3, torch.tensor([1]), torch.tensor([0]))\n",
        "\n",
        "    outputs = best_model(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "loss.item()\n",
        "\n",
        "#0.1478763073682785  epoch 2\n",
        "#0.07189879566431046 epoch 20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X61WzU30KkR8"
      },
      "source": [
        "2. Compute a threshold that meets the objectives of the larger task. Explain your reasoning clearly. This is an open question, but there are certain range of metrics that makes sense here. [5 marks]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXUKDaQCKjii"
      },
      "outputs": [],
      "source": [
        "# Your code here\n",
        "\n",
        "print(f\"New threshold is {THRESHOLD}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQg5NgZU5xfN"
      },
      "source": [
        "3. Build a cat photo capture system that stores the best image of the cat and blurs the background. We provided you with a randomly picked sample video.\n",
        "\n",
        "  For this assignment, we will define the best image as the one in which the cat occupies the most area on screen. So your objectives are\n",
        "    1. Iterate through every frame of video, use the classifer you trained with the threshold you designed to count the number of frames in which there is a cat. `[5 marks]`\n",
        "    2. If a cat is detected, segment the cat and enhance the image. The enhanced image could have either (do one of these).  `[15 marks]`\n",
        "      - Blurred background background. Use `GaussianBlur` function in opencv to achieve the blur effect.\n",
        "      - White background.      \n",
        "    3. Figure out a logic to find out the frame in which the cat occupies most area and save/display that image. The file name of that should be `cat_<current_time>_<number_of_frames>.jpg`. `[5 marks]`\n",
        "\n",
        "Notes and hints\n",
        "- The results here need not be perfect, you will have blurred image of the cat because of motion in the video. For the scope of this assignment, motion blur in the image is acceptable.\n",
        "- Use maskRCNN.\n",
        "- Use pretrained model from torch vision.  \n",
        "- Video itself is long, but for experimentation first 50 frames will be enough to get you some good samples of cats.\n",
        "- Try `sample_video2.mp4` first, then `sample_video.mp4`.\n",
        "- The videos are provided for educationl use only, from https://www.pexels.com/search/videos/cat%20jump/. Please donot redistrubute the sample videos.\n",
        "\n",
        "Here is some sample code to read the video file to get you started."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfKQrFMNX2Bl",
        "outputId": "947aaaa5-eff8-4a8b-f9fb-b05a23a8400f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File already exists! Delete the file to force download again...\n",
            "--2024-03-07 00:49:19--  https://docs.google.com/uc?export=download&id=1mkAjevnCeZDefXrDIu9N-2qIxDngPbL0&confirm=t\n",
            "Resolving docs.google.com (docs.google.com)... 142.251.2.102, 142.251.2.139, 142.251.2.101, ...\n",
            "Connecting to docs.google.com (docs.google.com)|142.251.2.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1mkAjevnCeZDefXrDIu9N-2qIxDngPbL0&export=download [following]\n",
            "--2024-03-07 00:49:19--  https://drive.usercontent.google.com/download?id=1mkAjevnCeZDefXrDIu9N-2qIxDngPbL0&export=download\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.250.101.132, 2607:f8b0:4023:c06::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.250.101.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2428 (2.4K) [text/html]\n",
            "Saving to: ‘sample_video.mp4’\n",
            "\n",
            "sample_video.mp4    100%[===================>]   2.37K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-03-07 00:49:19 (36.9 MB/s) - ‘sample_video.mp4’ saved [2428/2428]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# The code below minimizes the amount of downloads when you rerun all cells.\n",
        "# You probably don't need to download the same file over and over again...\n",
        "if os.path.isfile('sample_video2.mp4'):\n",
        "    print('File already exists! Delete the file to force download again...')\n",
        "else:\n",
        "    !wget -O sample_video2.mp4 'https://docs.google.com/uc?export=download&id=1wQDX5uu56NOLVPXQtngjeA4Cy5gygd_6&confirm=t'\n",
        "\n",
        "if os.path.isfile('sample_video.mp4'):\n",
        "    print('File already exists! Delete the file to force download again...')\n",
        "else:\n",
        "    !wget -O sample_video.mp4 'https://docs.google.com/uc?export=download&id=1mkAjevnCeZDefXrDIu9N-2qIxDngPbL0&confirm=t'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZc_5BaOuC-k"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import datetime\n",
        "\n",
        "TOTAL_FRAME_LIMIT = 2\n",
        "framecount = 0\n",
        "\n",
        "def blur_background(image, mask):\n",
        "    out_image = image\n",
        "    # your code here\n",
        "    return out_image\n",
        "\n",
        "# your model set up code here\n",
        "\n",
        "# Read sample file and process\n",
        "cap = cv2.VideoCapture(\"sample_video2.mp4\")\n",
        "if not cap.isOpened():\n",
        "    print(\"Error opening video file\")\n",
        "    exit(1)\n",
        "\n",
        "def process_frame(frame):\n",
        "    best_image = frame\n",
        "    best_mask = None\n",
        "    # single image processing code here\n",
        "    return best_image, best_mask\n",
        "\n",
        "print(\"Processing frames...\")\n",
        "for totalframe in tqdm(range(TOTAL_FRAME_LIMIT)):\n",
        "    framecount += 1\n",
        "    ret, frame = cap.read()\n",
        "    if ret:\n",
        "        best_image, best_mask= process_frame(frame)\n",
        "    else:\n",
        "        break\n",
        "\n",
        "best_image = blur_background(best_image, best_mask)\n",
        "best_image = cv2.cvtColor(best_image, cv2.COLOR_BGR2RGB)\n",
        "plt.imshow(best_image)\n",
        "plt.title(f\"cat_{datetime.datetime.now()}_{framecount}.jpg\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkLbkPJBPixG"
      },
      "source": [
        "Extra tasks if you are interested\n",
        "- Make this real time.  \n",
        "- Use image quality metric to take the frame with the best image of the cat\n",
        "- Improve segmentation with Segment anything model."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
